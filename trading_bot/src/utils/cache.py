"""
Gestionnaire de cache pour les donn√©es boursi√®res.
Migration de vos fonctions get_cached_data et preload_cache.
"""
import pandas as pd
import yfinance as yf
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor
import pickle

from config.settings import config
from .logger import get_logger

logger = get_logger(__name__)

class CacheManager:
    """
    Gestionnaire de cache pour les donn√©es boursi√®res.
    Migration directe de vos fonctions de cache.
    """

    def __init__(self):
        self.cache_dir = config.paths.cache_dir
        self.max_age_hours = config.trading.cache_max_age_hours
        self.max_workers = config.trading.max_workers

    def get_cached_data(self, symbol: str, period: str, 
                       max_age_hours: Optional[int] = None,
                       offline_mode: bool = False) -> pd.DataFrame:
        """
        R√©cup√®re les donn√©es en cache si elles existent et sont r√©centes, sinon t√©l√©charge.
        Migration directe de votre fonction get_cached_data().

        Args:
            symbol: Symbole boursier (ex: 'AAPL').
            period: P√©riode des donn√©es (ex: '1y').
            max_age_hours: √Çge maximum du cache en heures.
            offline_mode: Mode hors ligne.

        Returns:
            pd.DataFrame avec les donn√©es, ou DataFrame vide si √©chec.
        """
        if max_age_hours is None:
            max_age_hours = self.max_age_hours

        cache_file = self.cache_dir / f"{symbol}_{period}.pkl"

        # V√©rifier le cache existant
        if cache_file.exists():
            file_age = datetime.now() - datetime.fromtimestamp(cache_file.stat().st_mtime)
            age_hours = file_age.total_seconds() / 3600

            if age_hours < max_age_hours or offline_mode:
                try:
                    return pd.read_pickle(cache_file)
                except Exception as e:
                    logger.warning(f"Erreur lecture cache {symbol}: {e}")

        if offline_mode:
            # Si offline, charger le cache m√™me s'il est vieux
            if cache_file.exists():
                try:
                    return pd.read_pickle(cache_file)
                except Exception as e:
                    logger.error(f"Erreur cache offline {symbol}: {e}")
            else:
                logger.warning(f"Pas de cache disponible pour {symbol} ({period}) en mode hors ligne.")
            return pd.DataFrame()

        # Sinon, t√©l√©charger et mettre en cache
        try:
            logger.info(f"T√©l√©chargement {symbol} ({period})...")
            data = yf.download(symbol, period=period)

            if not data.empty:
                # Sauvegarder en cache
                cache_file.parent.mkdir(parents=True, exist_ok=True)
                data.to_pickle(cache_file)
                logger.debug(f"Cache sauvegard√©: {cache_file}")

            return data

        except Exception as e:
            logger.error(f"Erreur t√©l√©chargement {symbol}: {e}")

            # Essayer de charger un cache ancien
            if cache_file.exists():
                try:
                    logger.info(f"Utilisation cache ancien pour {symbol}")
                    return pd.read_pickle(cache_file)
                except Exception as cache_error:
                    logger.error(f"Erreur cache ancien {symbol}: {cache_error}")

            return pd.DataFrame()

    def preload_batch(self, symbols: List[str], period: str) -> None:
        """
        Pr√©-charge le cache pour une liste de symboles.
        Migration de votre fonction preload_cache().

        Args:
            symbols: Liste des symboles √† pr√©-charger.
            period: P√©riode des donn√©es.
        """
        logger.info(f"Pr√©-chargement du cache pour {len(symbols)} symboles...")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self.get_cached_data, symbol, period) 
                for symbol in symbols
            ]

            # Attendre que tous les t√©l√©chargements soient termin√©s
            completed = 0
            for future in futures:
                try:
                    result = future.result()
                    completed += 1
                    if completed % 10 == 0:
                        logger.info(f"Pr√©-chargement: {completed}/{len(symbols)} compl√©t√©s")
                except Exception as e:
                    logger.warning(f"Erreur pr√©-chargement: {e}")

        logger.info(f"‚úÖ Pr√©-chargement termin√©: {completed}/{len(symbols)} symboles")

    def clear_cache(self, older_than_days: int = 7) -> None:
        """
        Nettoie le cache des fichiers anciens.

        Args:
            older_than_days: Supprimer les fichiers plus anciens que N jours.
        """
        if not self.cache_dir.exists():
            return

        cutoff_date = datetime.now() - timedelta(days=older_than_days)
        removed_count = 0

        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                file_date = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_date < cutoff_date:
                    cache_file.unlink()
                    removed_count += 1
            except Exception as e:
                logger.warning(f"Erreur suppression cache {cache_file}: {e}")

        logger.info(f"üßπ Cache nettoy√©: {removed_count} fichiers supprim√©s")

    def get_cache_stats(self) -> dict:
        """Retourne des statistiques sur le cache."""
        if not self.cache_dir.exists():
            return {"total_files": 0, "total_size_mb": 0}

        total_files = 0
        total_size = 0

        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                total_files += 1
                total_size += cache_file.stat().st_size
            except Exception:
                continue

        return {
            "total_files": total_files,
            "total_size_mb": round(total_size / (1024*1024), 2),
            "cache_dir": str(self.cache_dir)
        }
